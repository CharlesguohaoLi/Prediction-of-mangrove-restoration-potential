import numpy as np
import pandas as pd
import geopandas as gpd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, cohen_kappa_score, confusion_matrix, accuracy_score
from joblib import dump
import random
from scipy.stats import chi2

# Load training dataset
exist_data = pd.read_csv('data/existing_samples.csv')
non_exist_data = gpd.read_file('data/non_existing_samples.gdb', layer='non_exist_layer')
print(f'Exist data loaded with {exist_data.shape[0]} samples.')
print(f'Non-exist data loaded with {non_exist_data.shape[0]} samples.')

# Load parameter file
best_params_df = pd.read_csv('data/grid_search_AUC.csv')
best_params = eval(best_params_df['Parameters'].iloc[0])
variables_per_split, min_leaf_population, max_nodes, n_trees = best_params
print(f'Evaluating parameters: {best_params}')

# Feature columns
feature_columns = exist_data.columns.drop('occur')

# Load prediction dataset
try:
    input_predict_table = gpd.read_file('data/prediction_samples.gdb', layer='prediction_layer')
except Exception as e:
    raise FileNotFoundError(f"Failed to read prediction dataset: {e}")

# Check coordinate reference system (CRS)
if input_predict_table.crs.to_string() != 'EPSG:4326':
    raise ValueError(f"Prediction dataset CRS is not GCS WGS 1984. Current CRS: {input_predict_table.crs}")

predict_feature_columns = input_predict_table.columns[input_predict_table.columns.isin(feature_columns)].tolist()
X_predict = input_predict_table[predict_feature_columns]

# Calculate Mahalanobis distance
def calculate_mahalanobis_distance(X, mean, inv_cov):
    diff = X - mean
    return np.sqrt(np.dot(np.dot(diff, inv_cov), diff.T))

# Check for extrapolation
def check_extrapolation(X, train_data, threshold=0.95):
    mean = np.mean(train_data, axis=0)
    cov = np.cov(train_data, rowvar=False)
    inv_cov = np.linalg.inv(cov)

    distances = np.array([calculate_mahalanobis_distance(x, mean, inv_cov) for x in X])
    chi2_threshold = chi2.ppf(threshold, df=X.shape[1])
    return distances > chi2_threshold

# Define Random Forest model
def train_rf(X_train, y_train):
    rf = RandomForestClassifier(
        n_estimators=n_trees, min_samples_leaf=min_leaf_population,
        max_features=variables_per_split, max_leaf_nodes=max_nodes,
        bootstrap=True, oob_score=True, random_state=60, n_jobs=-1
    )
    rf.fit(X_train, y_train)
    return rf

# Sample non-existing data, merge with existing data, train the model, and predict suitable areas
def sample_and_train(non_exist_data, exist_data, X_predict):
    all_predictions = []  # Initialize empty list to store prediction results
    all_metrics = []  # Store evaluation metrics for each iteration
    oob_scores = []  # Save OOB scores for each iteration
    all_prediction_types = []  # Store extrapolation type for each model

    for sample_idx in range(10):  # 10 sampling iterations
        print(f"Starting iteration {sample_idx + 1} for data merging and training")
        # Randomly sample non-existing data
        random_seed = 60 + sample_idx  # Use different seeds to ensure sampling diversity and reproducibility
        sampled_non_exist_data = non_exist_data.sample(n=exist_data.shape[0], replace=True, random_state=random_seed)
        non_exist_sample = sampled_non_exist_data[feature_columns].astype(exist_data[feature_columns].dtypes)

        # Merge data
        X_train_combined = pd.concat([exist_data[feature_columns], non_exist_sample])
        y_train_combined = pd.concat([exist_data['occur'], pd.Series([0] * len(non_exist_sample))])

        # Train the Random Forest model
        rf_model = train_rf(X_train_combined, y_train_combined)

        # Save the trained model
        model_filename = f'models/rf_model_{sample_idx + 1}.joblib'
        dump(rf_model, model_filename)  # Save model as .joblib file
        print(f'Model saved to {model_filename}')

        # Get OOB score
        oob_score = rf_model.oob_score_
        oob_scores.append(oob_score)

        # Make predictions
        print("Making predictions...")
        y_prob_predict = rf_model.predict_proba(X_predict)[:, 1]
        all_predictions.append(y_prob_predict)  # Store prediction probabilities

        # Calculate evaluation metrics using out-of-bag (OOB) samples
        y_prob_oob = rf_model.oob_decision_function_[:, 1]
        auc_score = roc_auc_score(y_train_combined, y_prob_oob)
        kappa_score = cohen_kappa_score(y_train_combined, y_train_pred)
        cm = confusion_matrix(y_train_combined, y_train_pred)
        tss = (cm[1, 1] / (cm[1, 1] + cm[1, 0])) - (cm[0, 1] / (cm[0, 1] + cm[0, 0]))
        accuracy = accuracy_score(y_train_combined, y_train_pred)
        q_stat = (cm[1, 1] * cm[0, 0] - cm[1, 0] * cm[0, 1]) / ((cm[1, 1] * cm[0, 0] + cm[1, 0] * cm[0, 1]))

        # Store metrics
        metrics_results = {
            "AUC": auc_score,
            "Kappa": kappa_score,
            "TSS": tss,
            "Accuracy": accuracy,
            "Q": q_stat,
            "OOB": oob_score
        }

        all_metrics.append(metrics_results)

        # Check for extrapolation
        is_extrapolation = check_extrapolation(X_predict.to_numpy(), X_train_combined.to_numpy())
        prediction_type = [2 if extrapolate else 1 for extrapolate in is_extrapolation]
        all_prediction_types.append(prediction_type)

    # Average predictions over all iterations
    mean_predictions = np.mean(all_predictions, axis=0)
    min_predictions = np.min(all_predictions, axis=0)
    max_predictions = np.max(all_predictions, axis=0)
    uncertainty = np.std(all_predictions, axis=0)  # Calculate uncertainty as standard deviation

    # Average evaluation metrics across all iterations
    mean_metrics = {metric: np.mean([metrics[metric] for metrics in all_metrics]) for metric in all_metrics[0]}
    mean_oob = np.mean(oob_scores)

    # Calculate final prediction types (extrapolation if any model flags it)
    combined_prediction_types = np.max(all_prediction_types, axis=0)

    return all_predictions, mean_predictions, mean_metrics, mean_oob, min_predictions, max_predictions, uncertainty, combined_prediction_types


# Perform predictions
all_predictions, mean_predictions, mean_metrics, mean_oob, min_predictions, max_predictions, uncertainty, prediction_types = sample_and_train(
    non_exist_data, exist_data, X_predict)

# Output average evaluation metrics
print(f"Average metrics over 10 runs: {mean_metrics}")
print(f"Average OOB score over 10 runs: {mean_oob}")

# Save evaluation metrics to CSV file
metrics_results_df = pd.DataFrame({
    "Metric": ["AUC", "Kappa", "TSS", "Accuracy", "Q", "OOB"],
    "Average Value": [
        mean_metrics["AUC"],
        mean_metrics["Kappa"],
        mean_metrics["TSS"],
        mean_metrics["Accuracy"],
        mean_metrics["Q"],
        mean_oob
    ]
})
metrics_results_df.to_csv('results/evaluation_metrics.csv', index=False)

# Set a fixed threshold of 0.5
threshold = 0.5
pred_mean = (mean_predictions >= threshold).astype(int)

# Add prediction results to the prediction dataset
input_predict_table['suit_mn'] = pred_mean
input_predict_table['o_mn'] = mean_predictions
input_predict_table['suit_min'] = (min_predictions >= threshold).astype(int)
input_predict_table['o_min'] = min_predictions
input_predict_table['suit_max'] = (max_predictions >= threshold).astype(int)
input_predict_table['o_max'] = max_predictions
input_predict_table['uncertainty'] = uncertainty  # Add uncertainty column
input_predict_table['extrapolation'] = prediction_types  # Add extrapolation flag

# Save the result as Shapefile in batches
batch_size = 1000000
num_batches = len(input_predict_table) // batch_size + 1
print(f"Writing {len(input_predict_table)} rows in {num_batches} batches")

for i in range(num_batches):
    start_idx = i * batch_size
    end_idx = (i + 1) * batch_size
    batch_data = input_predict_table.iloc[start_idx:end_idx]
    batch_filename = f'results/suitable_area_predictions_batch_{i + 1}.shp'
    batch_data.to_file(batch_filename, encoding='utf-8')
    print(f'Batch {i + 1} written to file.')
